추천 시스템의 최적의 결과를 위해서는 어떤 세팅이 더 좋은 결과를 가져오는지를 평가해야 한다. 추천 시스템의 정확도를 평가하는 방법은 여러 가지 있을 수 있지만 가장 널리 쓰이는 방법은 시뮬레이션(Simulation)이다. 이 방법에서는 데이터를 두 개의 세트로 나누고 하나의 데이터 세트(Training set)를 사용해서 다른 세트(Validation set)의 추천을 만들어낸다.(더 정확하게는 다른 세트의 아이템의 예상 평가값을 계산해 낸다.)
총 m 개의 아이템과 n명의 사용자가 있는데, 이 중에서 2명의 사용자와 3개의 아이템을 대상으로 정확도를 평가하기로 했다고 하자. 이 경우 2명의 사용자(User$_{n-1}$과 User$_{n}$)의 3개의 아이템(Item$_{m-2}$, Item$_{m-1}$, Item$_{m}$)에 대한 평가가 validation set 이고 나머지가 traning set이 된다. 우선, validation set이 없는 상태에서 training set만을 가지고 validation의 예상 평점을 계산한다. 이 경우, validation set의 실제 평점을 가지고 있기 때문에 이렇게 계산된 예상 평점과 실제 평점과의 차이를 계산할 수 있으며, 이것도 정확도라고 할 수 있다.
정확도는 validation set과 training set에 누가 들어가는지, validation set에 몇 명의 사용자와 몇 개의 아이템을 넣을지 등에 따라 달라 질 수 있기 때문에 더 정확한 평가를 위해 training set과 validation set의 구성을 바꿔가면서 정확도를 계산해 볼 필요가 있다.
정확도 측정은 크게 각 아이템의 예상 평점과 실제 평점의 차이를 계산하는 방법과 추천한 아이템과 사용자의 실제 선택을 비교하는 방법이 있다.
두 평점 간에 차이가 적을수록, 즉 아래 측정지표가 작을수록 정확하다고 볼 수 있다.
MAD(Mean absolute deviation) =
$\frac{\sum_{i=1}^{n}\sum_{j=1}^{m}(|r_{i,j}-p_{i,j}|if r_{i,j}\ \neq \ null,\ 0 \ otherwise)}{k}$

MSE(Mean squared error) =
$\frac{\sum_{i=1}^{m}\sum{j=1}^{m}((r_{i,j}-p_{i,j})^2\ if \ r_{i,j} \neq \ null,\ 0\ otherwise)}{k}$

RMSE(Root mean squared error) = $\sqrt{MSE}$

n: validation set에 있는 사용자의 수
m: validation set에 있는 아이템의 수
r$_{i,j}$: 사용자 i의 아이템 j에 대한 실제 평점
p$_{i,j}$:사용자 i의 아이템 j에 대한 예상 평점
k: 정확도 계산에 포함된 총 아이템 수

다음으로 추천한 아이템과 사용자가 실제 선택한 아이템을 비교하는 방법은 데이터가 이진수(예를들어 선택 여부)인 경우에는 예상 평점은 계산이 가능 하지만 실제 평점과 비교할 수가 없다. 그래서 이진수의 경우에는 추천 아이템과 사용자가 실제 선택한 아이템을 비교해야 한다. 위에서 설명한 기준에 따라 추천 아이템이 결정되면 이를 사용자의 실제 선택과 비교한다. 이때 추천 시스템의 성능을 평가하는 중요한 두 가지 측정지표는 정확도(precision)와 재현율(recall)이다. 이 두 지표는 문헌정보학에서 유래된 것으로 각 측정 지표의 식은 아래에 있다.

정확도(Precision, Accurarcy) = 사용자가 실제 선택한 아이템수 / 전체 추천된 아이템 수
재현율(Recall): 맞는 추천 아이템 수 / 사용자가 선택한 전체 아이템 수
F 측정치(F measure): (2 X 정확도 X 재현율) / (정확도 + 재현율)
범위(Coverage) = 추천이 가능한 사용자 수(혹은 아이템 수) / 전체 사용자 수(혹은 아이템 수)

정확도는 추천 시스템이 추천한 아이템 중 실제 사용자가 좋아하는 아이템의 비율이다. 예를 들어 추천 시스템이 10개를 추천했는데 validation set의 실제 데이터를 봤더니 사용자가 실제로 선택한(구매 혹은 클릭한) 아이템이 6개 였다면 정확도는 0.6(=6/10)이 된다.
재현율은 사용자가 실제로 좋아하는 아이템 중에서 몇 개의 아이템을 맞췄는가 하는 것이다. 위와 같은 예에서 validation set을 보니 사용자가 선택한 아이템의 수가 총 12개였다면 이 중 6개를 맞췄으므로 재현율은 0.5가 된다.
정확도와 재현율은 서로  반대의 관계(trade-off)가 있다. 재현율을 높이기 위해서 추천하는 아이템의 수를 늘리면 정확도가 낮아지고 정확도를 높이기 위해 추천하는 아이템의 수를 줄이면 재현율이 낮아지기 때문이다. 그래서 정확도와 재현율을 동시에 고려하는 F 측정치를 많이 사용한다.
범위는 전체 사용자(혹은 아이템) 중에서 추천이 가능한 사용자(아이템)의 비율을 말한다. 사용자가 평가한 아이템의 수가 적다면 충분한 이웃 사용자를 확보하지 못해서 추천을 못하는 경우가 생길 수 있다. 예를 들어, Neighbor 크기를 최적의 크기보다 줄이면 추천 가능한 사용자가 많아져서 범위는 높아지지만 정확도는 떨어진다. Item-based CF의 경우는 전처 아이템 중에서 다른 아이템과 유사도를 계산할 수 있는 아이템의 비율이 범위가 된다. 사용자와 마찬가리도 평가의 수가 적은 아이템은 유사도를 계산할 수 없는 경우가 있다. 그래서, 기준을 낮추면(유사도 계산을 위한 최소 사용자의 수)범위는 올라가지만 정확도는 떨어진다.

어떤 추천 시스템이 한 사용자를 대상으로 추천을 하는 경우에 Validation set에 있는 전체 아이템(a+b+c+d) 중에서 그 추천 시스템이 b+d를 사용자가 선택했을 것이라고 예상하고 이를 추천했다면, 그 사용자가 실제로 선택한 아이템은 c+d이다. 이때 정확도는 시스템이 정확하게 분류한 비율이므로 정확히 분류한 아이템인 a(추천하지 않았는데 사용자도 실제로 선택 안함)와 d(추천했는데 사용자가 실제로 선택)의 수를 전체 아이템수(a+b+c+d)로 나눈 것이 된다. 재현율은 사용자가 실제로 선택한 아이템의 수(c+d) 중 시스템이 맞춤 아이템의 수(d)의 비율이 된다. 여기에서 또 다른 용어로는 True positive recommendation(TPR:추천 시스템이 추천한 아이템 중 사용자가 선택한 아이템의 비율)과 False positive recommendatoin(FPR: 추천 시스템이 추천했는데 사용자가 선택하지 않은 아이템의 비율)이다. 여기에서 TPR은 precision과 같은 개념이다. 일반적으로 대상 아이템이 유한한 경우 정확도 개념을 사용하고 대상 아이템이 무수히 많은 경우(즉, a의 수를 알 수 없는 경우)에는 앞에서 설명한 정확도 개념을 사용한다.

|actual \ predicted   |negative   |positive   |
|---|---|---|
|negative   |a   |b   |
|positive   |c   |d   |
<추천 시스템의 성과 측정 지표>
Precision = $\frac{a+d}{a+b+c+d}$
Recall = $\frac{d}{c+d}$
TPR(True positive recommendation) = ${\frac{d}{b+d}}$\
FPR(False positive recommendation) = ${\frac{b}{b+d}}$
